1 了解什么是Machine learning

机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。
专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。
它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。

2 学习中心极限定理，学习正态分布，学习最大似然估计

2.1 中心极限定理

中心极限定理指的是给定一个任意分布的总体。我每次从这些总体中随机抽取 n 个抽样，一共抽 m 次。 然后把这 m 组抽样分别求出平均值。 
这些平均值的分布接近正态分布。

ref： https://zhuanlan.zhihu.com/p/25241653

2.2 正态分布

正态分布（Normal distribution），也称“常态分布”，又名高斯分布（Gaussian distribution），是一个在数学、物理及工程等领域都非常重要的概率分布，
在统计学的许多方面有着重大的影响力。
正态曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，因此人们又经常称之为钟形曲线。

若随机变量X服从一个数学期望为μ、方差为σ^2的正态分布，记为N(μ，σ^2)。其概率密度函数为正态分布的期望值μ决定了其位置，其标准差σ决定了分布的幅度。
当μ = 0,σ = 1时的正态分布是标准正态分布。

2.3 最大似然估计

极大似然估计，通俗理解来说，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！

换句话说，极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。

极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的。

3 推导回归Loss function

4 学习损失函数与凸函数之间的关系

凸函数是有且只有全局最优解的，而非凸函数可能有多个局部最优解。
针对逻辑回归、线性回归这样的凸函数，使用梯度下降或者牛顿法可以求出参数的全局最优解，针对神经网络这样的非凸函数，我们可能会找到许多局部最优解。

5 了解全局最优和局部最优

局部最优，是指对于一个问题的解在一定范围或区域内最优，或者说解决问题或达成目标的手段在一定范围或限制内最优。

针对一定条件/环境下的一个问题/目标，若一项决策和所有解决该问题的决策相比是最优的，就可以被称为全局最优。

6 学习导数，泰勒展开


7 推导梯度下降公式


8 写出梯度下降的代码

// An highlighted block
import numpy as np

np.random.seed(1)
x = np.linspace(0,10,10)
y_label = 2*x +3

def gradient_descent(x, y ,w, b):
    time = 0
    n=0.0001
    w = np.random.randn(1,1)
	b = np.random.randn(1,1)
    loss = np.sum((y - (w*x + b))**2)
    while(loss>0.01)or(time<10000):
        graident_w = np.sum((y - (w*x + b))*(x))
        graident_b = np.sum(y - (w*x + b))
        w = w - n*graident_w
        b = b - n*graident_b
        loss = np.sum((y - (w*x + b))**2)
        print(loss)
    return w, b
    
w_u, b_u = gradient_descent(x, y_label, w, b)

ref：https://blog.csdn.net/Jdrops00/article/details/90178511

9 学习L2-Norm，L1-Norm，L0-Norm


10 推导正则化公式


11 说明为什么用L1-Norm代替L0-Norm

L0是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。换句话说，让参数W是稀疏的。

但不幸的是，L0范数的最优化问题是一个NP hard问题，而且理论上有证明，L1范数是L0范数的最优凸近似，因此通常使用L1范数来代替。

12 学习为什么只对w/Θ做限制，不对b做限制

一般而言，在神经网络中的正则化项只对权重 w 做，而不对偏置 b 做，精确拟合偏置所需的数据通常比拟合权重少得多。
这是因为每个权重 w 会指定两个变量如何相互作用，而每个偏置只控制单一变量，这意味着我们不对其进行正则化也不会导致太大的方差。
另外正则化偏置参数可能会导致明显的欠拟合。

因为做限制的原因是防止模型对当前的训练数据过拟合进而失去泛化能力，而过拟合的表现就是模型为了符合当前数据的分布情况趋向于变得复杂以获得更低的loss。
而模型的复杂程度是由w/θ决定的，b只是起到平移模型的作用。缩小b不能使模型简化，只能使模型分界面趋于靠近原点。
