# datawhale-LHY-study

# Task 2

学习视频内容：

观看观看李宏毅课程内容：P4、P5、P6、P7

视频连接：

https://www.bilibili.com/video/av35932863?from=search&seid=8120828691691969718

学习打卡内容：

    理解偏差和方差
    学习误差为什么是偏差和方差而产生的，并且推导数学公式
    过拟合，欠拟合，分别对应bias和variance什么情况
    学习鞍点，复习上次任务学习的全局最优和局部最优
    解决办法有哪些
    梯度下降
    学习Mini-Batch与SGD
    学习Batch与Mini-Batch，SGD梯度下降的区别
    如何根据样本大小选择哪个梯度下降(批量梯度下降，Mini-Batch）
    写出SGD和Mini-Batch的代码
    学习交叉验证
    学习归一化 
    学习回归模型评价指标
 
更多的对梯度下降优化将在《李宏毅深度学习》中会有学习任务介绍(指数加权平均，动量梯度下降等)

学习任务说明：

    建立CSDN账号或者简书等开源账号，将学习打卡任务写到CSDN或者简单等开源平台上
    未按时打卡这将会被清退
    
打卡地点：李宏毅机器学习第七群

打卡形式：表格

https://docs.qq.com/sheet/DRU54aFRwQ29ZS2RD?id=DRU54aFRwQ29ZS2RD&coord=E3%24E3%240%240%240%240&tab=BB08J2

参考内容：

    李宏毅机器学习课程
    Datawhale整理开源笔记《李宏毅机器学习》
    https://datawhalechina.github.io/Leeml-Book

参考链接：

参考数学：学习第三天会公布(5.16)

http://note.youdao.com/noteshare?id=d3473fb363e58beb9e36fd3d18f5f1f9&sub=WEBf02d214af85ff84fb9852f785e9a848b

参考代码链接：学习第三天我会公布（5.16）

Task2.py
4.3KB

参考文档：这次我(负责人)会自己写一份作业，和小伙伴们一起学习

http://note.youdao.com/noteshare?id=d3473fb363e58beb9e36fd3d18f5f1f9&sub=WEBf02d214af85ff84fb9852f785e9a848b

在第二天给出数学参考是因为：更多想把时间留给学习者，我所写的只是参考，希望大家有自己的想法

优秀学员链接：
追风者：https://blog.csdn.net/Crafts_Neo/article/details/90268784

# Task 4

学习视频内容：

观看观看李宏毅课程内容：p8

视频连接：

https://www.bilibili.com/video/av35932863/?p=8

学习Datawhale整理笔记

https://datawhalechina.github.io/Leeml-Book/#/chapter8/chapter8(目前已100%复现)

https://datawhalechina.github.io/Leeml-Book/#/chapter8/chapter8

说明：

笔记内容将会随着学习任务进行放出

学习打卡内容：

从基础概率推导贝叶斯公式，朴素贝叶斯公式(1)

学习先验概率(2)

学习后验概率(3)

学习LR和linear regreeesion之间的区别(4)

推导sigmoid function公式(5)

要求：
我将将会对打卡链接进行查看，要求打卡学习任务不得少于3个(上述学习内容序号)

如果少于3个学习任务将会被清退

可以晚点提交并说明情况，如果出现假打卡情况，立即清退。

学习任务说明：

 建立CSDN账号或者简书等开源账号，将学习打卡任务写到CSDN或者简单等开源平台上
 
未按时打卡这将会被清退

打卡地点：李宏毅机器学习第七群

打卡形式：表格

https://docs.qq.com/sheet/DRU54aFRwQ29ZS2RD?opendocxfrom=admin&id=DRU54aFRwQ29ZS2RD&coord=E1%24E1%242%240%240%240&tab=BB08J2

参考链接：

学习Datawhale整理笔记

https://datawhalechina.github.io/Leeml-Book/#/chapter8/chapter8

5.25，我将给出部分参考链接

http://note.youdao.com/noteshare?id=47ee5998b8abe6e51f3a587ba547bbdf&sub=A22B858671F84836A1BFA2CBC4F58284

晚点给出参考链接，更多想把时间留给学习者，我所写的只是参考，希望大家有自己的想法

# Task 5

推导LR损失函数(1)

学习LR梯度下降(2)

利用代码描述梯度下降(选做)(3)

Softmax原理(4)

softmax损失函数(5)

softmax梯度下降(6)
