# 1. 总结决策树模型结构

决策树是一类常见的机器学习方法，通过历史数据得到的树结构模型对新数据进行分类决策。

决策树是一种基于实例的归纳学习方法，它能从给定的无序的训练样本中，提炼出树形的分类模型。树中的每个非叶子节点记录了使用哪个特征来进行类别的判断，每个叶子节点则代表了最后判断的类别。根节点到每个叶子节点均形成一条分类的路径规则。

每次判断都是对某个属性的测试，每次判断都会缩小考虑的范围。

一棵决策树包含一个根节点、若干个内部结点和若干个叶节点；叶节点对应于决策结果，其他

每个节点则对应于一个属性测试；每个节点包含的样本集合根据属性测试的结果被划分到子节点；

根节点包含样本全集。从根节点到每个叶节点的路径对应了一个判断测试序列。

决策树的生成是一个递归的过程。有三种情形不会再分类。

⑴当前节点包含的样本全属于同一类别。⑵当前属性集为空。⑶当前节点的样本集合为空。

对于第二种情形，其类别设定为结点所含样本最多的类别。对于第三种情形，将其类别设定为其父节点所含样本最多的类别。

# 2. 学习信息增益

“信息熵”是度量样本集合纯度最常用的一种指标。假定当前样本集合D中第K类样本所占的比例为PK，则D的信息熵定义为

 ![equation1](https://github.com/npk123/datawhale-LHY-ongoing/blob/master/pics/xinxishang.jpg)
  
 ![equation1](https://github.com/npk123/datawhale-LHY-ongoing/blob/master/pics/xinxizengyi.jpg)

ID3决策树学习算法就是以信息增益为准则来选择划分属性的。

# 3. 学习信息增益率

实际上信息增益准则对可取值数目较多的属性有所偏好，为了减少这种偏好可能带来的不利影响。使用信息增益率来选择最优划分属性。

 ![equation1](https://github.com/npk123/datawhale-LHY-ongoing/blob/master/pics/xinxizengyilv.jpg)

C4.5算法常使用信息增益率来选择最优属性划分

# 4. 学习ID3算法优缺点

对CLS算法的最大改进是摒弃了属性选择的随机性，利用信息熵的下降速度作为属性选择的度量。ID3是一种基于信息熵的决策树分类学习算法，以信息增益和信息熵，作为对象分类的衡量标准。

ID3算法结构简单、学习能力强、分类速度快适合大规模数据分类。但同时由于信息增益的不稳定性，容易倾向于众数属性导致过度拟合，算法抗干扰能力差。

ID3算法缺点：倾向于选择那些属性取值比较多的属性，在实际的应用中往往取值比较多的属性对分类没有太大价值、不能对连续属性进行处理、对噪声数据比较敏感、需计算每一个属性的信息增益值、计算代价较高。

# 5. 学习C4.5算法优缺点

基于ID3算法的改进，主要包括：使用信息增益率替换了信息增益下降度作为属性选择的标准；在决策树构造的同时进行剪枝操作；避免了树的过度拟合情况；可以对不完整属性和连续型数据进行处理，提升了算法的普适性。

# 6. 学习决策树如何生成

决策树一般使用递归的方法生成。

寻找一棵最优的决策树是一个NP难题，因此只能采用一种基于启发式的贪心策略来寻找最优决策树。

决策树的生长过程主要有如下几步(假设训练数据集为D,特征集为A):

    若D中所有实例都属于同一个类别(或方差小于预设阈值)，则停止生长，返回决策树T.
    若A=∅，则将D中数量最多的类别(平均值)作为该节点的标记,返回T.
    若A≠∅，则从A中选择一个最优的特征Ai进行子节点的分裂(利用上面所说的分裂标准).分裂得到n个子节点，用Di表示。
    遍历每一个子节点，将Di作为新的D，从第一步开始重复上述操作。

Ref：https://blog.csdn.net/xuelabizp/article/details/50979469 

划分数据集代码

选择最好的数据集划分方式代码

创建树的函数代码

