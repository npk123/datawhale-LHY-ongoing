1 理解偏差和方差

偏差.

这里的偏指的是 偏离 , 那么它偏离了什么到导致了误差? 潜意识上, 当谈到这个词时, 我们可能会认为它是偏离了某个潜在的 “标准”, 
而这里这个 “标准” 也就是真实情况 (ground truth). 在分类任务中, 这个 “标准” 就是真实标签 (label).

方差.

很多人应该都还记得在统计学中, 一个随机变量的方差描述的是它的离散程度, 也就是该随机变量在其期望值附近的 波动程度 . 

2 学习误差为什么是偏差和方差而产生的，并且推导数学公式

误差为偏差、方差与噪声之和

3 过拟合，欠拟合，分别对应bias和variance什么情况

一般来说，简单的模型会有一个较大的偏差和较小的方差，复杂的模型偏差较小方差较大。

欠拟合：模型不能适配训练样本，有一个很大的偏差。

举个例子：我们可能有本质上是多项式的连续非线性数据，但模型只能表示线性关系。在此情况下，我们向模型提供多少数据不重要，
因为模型根本无法表示数据的基本关系，模型不能适配训练样本，有一个很大的偏差，因此我们需要更复杂的模型。
那么，是不是模型越复杂拟合程度越高越好呢？也不是，因为还有方差。

过拟合：模型很好的适配训练样本，但在测试集上表现很糟，有一个很大的方差。

方差就是指模型过于拟合训练数据，以至于没办法把模型的结果泛化。而泛化正是机器学习要解决的问题，如果一个模型只能对一组特定的数据有效，
换了数据就无效，我们就说这个模型过拟合。这就是模型很好的适配训练样本，但在测试集上表现很糟，有一个很大的方差。

4 学习鞍点，复习上次任务学习的全局最优和局部最优

一个不是局部最小值的驻点（一阶导数为0的点）称为鞍点。数学含义是： 目标函数在此点上的梯度（一阶导数）值为 0， 但从改点出发的一个方向是函数的极大值点，而在另一个方向是函数的极小值点。

因为深度学习中鞍点的大量存在，传统的牛顿法不适合，来寻优，因为牛顿法是通过直接寻找梯度为0的点，来寻优的,那么极有可能陷入鞍点。 
（ps: 也正因为如此，牛顿法在Hessian为正定的时候，比梯度下降速度快，因为牛顿法直接找梯度为0 的点，而梯度下降则是一次一次的寻找当前点的最优梯度）

Ref: https://blog.csdn.net/BVL10101111/article/details/78051939

5 解决办法有哪些

    (1）利用Hessian矩阵，判断是否为鞍点，因为，Hessian在鞍点具有正负特征值，而在局部最小值点正定。
    (2）随机梯度，相当于给正确的梯度加了一点noise，一定程度上避免了鞍点（但是只是一定程度） 
    (3）随机初始化起点，也有助于逃离鞍点
    (4）增加偶尔的随机扰动

6 梯度下降

梯度下降是目前神经网络中使用最为广泛的优化算法之一。为了弥补朴素梯度下降的种种缺陷，研究者们发明了一系列变种算法，
从最初的 SGD (随机梯度下降) 逐步演进到 NAdam。然而，许多学术界最为前沿的文章中，
都并没有一味使用 Adam/NAdam 等公认“好用”的自适应算法，很多甚至还选择了最为初级的 SGD 或者 SGD with Momentum 等。

ref:https://zhuanlan.zhihu.com/p/32626442

7 学习Mini-Batch与SGD

朴素 SGD (Stochastic Gradient Descent) 最为简单，没有动量的概念，即

mini-batch用了一些小样本来近似全部的，其本质就是竟然1个样本的近似不一定准，那就用更大的30个或50个样本来近似。
将样本分成m个mini-batch，每个mini-batch包含n个样本；在每个mini-batch里计算每个样本的梯度，
然后在这个mini-batch里求和取平均作为最终的梯度来更新参数；然后再用下一个mini-batch来计算梯度，
如此循环下去直到m个mini-batch操作完就称为一个epoch结束。


9 如何根据样本大小选择哪个梯度下降(批量梯度下降，Mini-Batch）


10 写出SGD和Mini-Batch的代码



学习交叉验证


学习归一化 


学习回归模型评价指标
